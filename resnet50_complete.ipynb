{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tNZaRpSFi6LT"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PKeuXVNERsc-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from torch import nn\n",
        "from torch.optim import Adam,RMSprop\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import random\n",
        "from torchvision import models\n",
        "from torch.nn import TripletMarginWithDistanceLoss\n",
        "import seaborn as sns\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import sklearn"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preprocessing Functions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CRAFT algorithm for  getting bounding boxes around the text part"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Source - https://github.com/clovaai/CRAFT-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from CRAFTpytorchmaster.craft import CRAFT\n",
        "from CRAFTpytorchmaster import imgproc\n",
        "from CRAFTpytorchmaster import craft_utils\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def copyStateDict(state_dict):\n",
        "    '''Utility Function for CRAFT'''\n",
        "    if list(state_dict.keys())[0].startswith(\"module\"):\n",
        "        start_idx = 1\n",
        "    else:\n",
        "        start_idx = 0\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        name = \".\".join(k.split(\".\")[start_idx:])\n",
        "        new_state_dict[name] = v\n",
        "    return new_state_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_net(net, image, text_threshold, link_threshold, low_text, cuda, poly, refine_net=None):\n",
        "    '''Utility function for craft'''\n",
        "    # resize\n",
        "    img_resized, target_ratio, size_heatmap = imgproc.resize_aspect_ratio(image, 1280, interpolation=cv2.INTER_LINEAR, mag_ratio=1.5)\n",
        "    ratio_h = ratio_w = 1 / target_ratio\n",
        "\n",
        "    # preprocessing\n",
        "    x = imgproc.normalizeMeanVariance(img_resized)\n",
        "    x = torch.from_numpy(x).permute(2, 0, 1)    # [h, w, c] to [c, h, w]\n",
        "    x = Variable(x.unsqueeze(0))                # [c, h, w] to [b, c, h, w]\n",
        "    if cuda:\n",
        "        x = x.cuda()\n",
        "\n",
        "    # forward pass\n",
        "    with torch.no_grad():\n",
        "        y, feature = net(x)\n",
        "\n",
        "    # make score and link map\n",
        "    score_text = y[0,:,:,0].cpu().data.numpy()\n",
        "    score_link = y[0,:,:,1].cpu().data.numpy()\n",
        "\n",
        "    # refine link\n",
        "    if refine_net is not None:\n",
        "        with torch.no_grad():\n",
        "            y_refiner = refine_net(y, feature)\n",
        "        score_link = y_refiner[0,:,:,0].cpu().data.numpy()\n",
        "\n",
        "\n",
        "    # Post-processing\n",
        "    boxes, polys = craft_utils.getDetBoxes(score_text, score_link, text_threshold, link_threshold, low_text, poly)\n",
        "\n",
        "    # coordinate adjustment\n",
        "    boxes = craft_utils.adjustResultCoordinates(boxes, ratio_w, ratio_h)\n",
        "    polys = craft_utils.adjustResultCoordinates(polys, ratio_w, ratio_h)\n",
        "    for k in range(len(polys)):\n",
        "        if polys[k] is None: polys[k] = boxes[k]\n",
        "\n",
        "    \n",
        "    # render results (optional)\n",
        "    render_img = score_text.copy()\n",
        "    render_img = np.hstack((render_img, score_link))\n",
        "    ret_score_text = imgproc.cvt2HeatmapImg(render_img)\n",
        "\n",
        "\n",
        "    return boxes, polys, ret_score_text\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_text_part(path):\n",
        "    '''Takes image as input and returns cropped image with text'''\n",
        "    net = CRAFT()\n",
        "    model_path = r\"C:\\Users\\Shreyas Vaidya\\Downloads\\craft_mlt_25k.pth\"\n",
        "    net.load_state_dict(copyStateDict(torch.load(model_path, map_location='cpu')))\n",
        "    net.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        img = imgproc.loadImage(path)\n",
        "        bboxes, _1, _2 = test_net(net, img, 0.7, 0.4, 0.4, cuda = False, poly =False)\n",
        "        y_min = 30000\n",
        "        x_min = 30000\n",
        "        y_max =-1\n",
        "        x_max = -1\n",
        "        for i in bboxes:\n",
        "            for x,y in i:\n",
        "                y_min = int(min(y,y_min))\n",
        "                x_min =int( min(x,x_min))\n",
        "                x_max =int( max(x,x_max))\n",
        "                y_max =int( max(y,y_max))\n",
        "\n",
        "    return img[y_min:y_max,x_min:x_max,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#test\n",
        "plt.imshow(extract_text_part(r\"D:\\Shreyas\\Shreyas_study\\vision_Challenges\\dataset\\dataset\\train\\M0021\\A1.jpg\"),cmap = 'gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.shape(extract_text_part(r\"D:\\Shreyas\\Shreyas_study\\vision_Challenges\\dataset\\dataset\\train\\M0021\\A1.jpg\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Otsu algoritm for binarisation of image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def otsu(image):\n",
        "    ret2,th2 = cv2.threshold(image,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
        "    return th2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.imshow(otsu(extract_text_part(r\"D:\\Shreyas\\Shreyas_study\\vision_Challenges\\dataset\\dataset\\train\\M0021\\A1.jpg\")),cmap = 'gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.shape(cv2.cvtColor(otsu(extract_text_part(r\"D:\\Shreyas\\Shreyas_study\\vision_Challenges\\dataset\\dataset\\train\\M0021\\A1.jpg\")),cv2.COLOR_GRAY2RGB))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detecting Language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tesserocr import PyTessBaseAPI\n",
        "\n",
        "def langdetect(path):\n",
        "    images = ['b.jpg']\n",
        "    images[0] = path\n",
        "    count = 0\n",
        "    count2 = 0\n",
        "    \n",
        "    with PyTessBaseAPI(path =r\"C:\\Program Files\\Tesseract-OCR\\tessdata\" ,lang = \"hin\") as api:\n",
        "        for img in images:\n",
        "            api.Init()\n",
        "            api.SetImageFile(img)\n",
        "            # print api.AllWordConfidences()\n",
        "            arr = list(api.AllWordConfidences())\n",
        "            sumarr = sum(arr) / float(max(len(arr),0.0001))\n",
        "\n",
        "\n",
        "    with PyTessBaseAPI(path =r\"C:\\Program Files\\Tesseract-OCR\\tessdata\" ,lang = \"eng\") as api:\n",
        "        for img in images:\n",
        "            api.Init()\n",
        "            api.SetImageFile(img)\n",
        "            # print api.AllWordConfidences()\n",
        "            arr2 = list(api.AllWordConfidences())\n",
        "            sumarr2 = sum(arr2) / float(max(len(arr2),0.0001))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    n = min(len(arr) , len(arr2))\n",
        "    for i in range(0 , n):\n",
        "        if (arr[i] > arr2[i]):\n",
        "            count += 1\n",
        "        elif (arr2[i] > arr[i]):\n",
        "            count2 += 1\n",
        "        \n",
        "        else:\n",
        "            pass\n",
        "\n",
        "\n",
        "\n",
        "    if (count2 > count):\n",
        "            lang = \"English\"\n",
        "            conf = sumarr2\n",
        "            api.Init(lang = 'eng')\n",
        "            api.SetImageFile(images[0])\n",
        "            return(lang,conf)\n",
        "    else:\n",
        "            lang = \"Hindi\"\n",
        "            conf = sumarr\n",
        "            api.Init(lang = 'hin')\n",
        "            api.SetImageFile(images[0])\n",
        "            return (lang,conf)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = r'D:\\Shreyas\\Shreyas_study\\vision_Challenges\\dataset\\dataset\\train'\n",
        "authors = os.listdir(path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#PrePRocessing train images\n",
        "newpath = r'D:\\Shreyas\\Shreyas_study\\vision_Challenges\\dataset\\dataset\\preprocessedTrain'\n",
        "\n",
        "for i,author_name in tqdm(enumerate(authors)):\n",
        "    images = os.listdir(os.path.join(path,author_name))\n",
        "    label = i\n",
        "    newimgpath = os.path.join(newpath,str(label))\n",
        "    os.mkdir(newimgpath)\n",
        "    os.chdir(newimgpath)\n",
        "    for j in images:\n",
        "        try:\n",
        "            img = otsu(extract_text_part(os.path.join(path,author_name,j)))\n",
        "            img = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n",
        "            cv2.imwrite(j,img)\n",
        "        except:\n",
        "            pass\n",
        "     \n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "oldvalpath = \"D:/Shreyas/Shreyas_study/vision_Challenges/dataset/dataset/val\"\n",
        "newvalpath = r'D:\\Shreyas\\Shreyas_study\\vision_Challenges\\dataset\\dataset\\preprocessedVal'\n",
        "os.mkdir(newvalpath)\n",
        "fail_count = 0\n",
        "for i in tqdm(os.listdir(oldvalpath)):\n",
        "        try:\n",
        "                img = otsu(extract_text_part(os.path.join(oldvalpath,i)))\n",
        "                img = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "        except:\n",
        "                img = otsu(cv2.imread(os.path.join(oldvalpath,i),cv2.IMREAD_GRAYSCALE))\n",
        "                img = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "        cv2.imwrite(os.path.join(newvalpath,i),img)\n",
        "    \n",
        "         \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = r'D:\\Shreyas\\Shreyas_study\\vision_Challenges\\dataset\\dataset\\preprocessedTrain'\n",
        "authors = os.listdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1352it [1:43:02,  4.57s/it]\n"
          ]
        }
      ],
      "source": [
        "same_pairs_list = []# List with 2 authors from same class and class label,will be used as anchor and positive\n",
        "for i,author_name in tqdm(enumerate(authors)):\n",
        "    images = os.listdir(os.path.join(path,author_name))\n",
        "    \n",
        "    label = i\n",
        "    for j in range(0,len(images)-1):\n",
        "        list_to_append = [os.path.join(path,author_name,images[j]),os.path.join(path,author_name,images[j+1]),label]\n",
        "        if(langdetect(os.path.join(path,author_name,images[j]))[0]!=\"Hindi\" or langdetect(os.path.join(path,author_name,images[j]))[1]<=1e-2 or langdetect(os.path.join(path,author_name,images[j+1]))[0]!=\"Hindi\" or langdetect(os.path.join(path,author_name,images[j+1]))[1]<=1e-2):\n",
        "            for i in range(10):\n",
        "                same_pairs_list.append(list_to_append)\n",
        "        else:\n",
        "            same_pairs_list.append(list_to_append)\n",
        "    try:\n",
        "        if(langdetect(os.path.join(path,author_name,images[-2]))[0]!=\"Hindi\" or langdetect(os.path.join(path,author_name,images[-2]))[1]<=1e-2 or langdetect(os.path.join(path,author_name,images[-1]))[0]!=\"Hindi\" or langdetect(os.path.join(path,author_name,images[-1]))[1]<=1e-2):\n",
        "            for i in range(10):\n",
        "                same_pairs_list.append([os.path.join(path,author_name,images[-2]),os.path.join(path,author_name,images[-1]),label])\n",
        "        else:\n",
        "            same_pairs_list.append([os.path.join(path,author_name,images[-2]),os.path.join(path,author_name,images[-1]),label])\n",
        "\n",
        "    except:\n",
        "        pass  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['D:\\\\Shreyas\\\\Shreyas_study\\\\vision_Challenges\\\\dataset\\\\dataset\\\\preprocessedTrain\\\\0\\\\A0.jpg',\n",
              " 'D:\\\\Shreyas\\\\Shreyas_study\\\\vision_Challenges\\\\dataset\\\\dataset\\\\preprocessedTrain\\\\0\\\\A1.jpg',\n",
              " 0]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "same_pairs_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "68233"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(same_pairs_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle \n",
        "with open (\"same_pairs_list.pickle\",\"wb\") as file:\n",
        "    pickle.dump(same_pairs_list,file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle \n",
        "with open (\"same_pairs_list.pickle\",\"rb\") as file:\n",
        "    same_pairs_list = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1352it [1:05:30,  2.91s/it]\n"
          ]
        }
      ],
      "source": [
        "images_list = [] #will be used for the negative in triplet loss\n",
        "for i,author_name in tqdm(enumerate(authors)):\n",
        "    images = os.listdir(os.path.join(path,author_name))\n",
        "    random.shuffle(images)\n",
        "    label = i\n",
        "    for j in images:\n",
        "        path_to_append = os.path.join(path,author_name,j)\n",
        "        if langdetect(path_to_append)[0]!=\"Hindi\" or langdetect(path_to_append)[1]<=1e-2:\n",
        "            for _ in range(10):\n",
        "                images_list.append([path_to_append,label])\n",
        "        else:\n",
        "            images_list.append([path_to_append,label])\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle \n",
        "with open (\"images_list.pickle\",\"wb\") as file:\n",
        "    pickle.dump(images_list,file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle \n",
        "with open (\"images_list.pickle\",\"rb\") as file:\n",
        "    images_list = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "47112"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(images_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1352it [00:01, 791.08it/s]\n"
          ]
        }
      ],
      "source": [
        "#creating list of image pairs with combinations and labels of 1 or 0\n",
        "\n",
        "imgp_label_list = []\n",
        "\n",
        "count = 0\n",
        "authors = os.listdir(path)\n",
        "for i,author_name in tqdm(enumerate(authors)):\n",
        "  other_list = authors[i:i+2] if i<len(authors)-1 else authors[i:]\n",
        "  for author_name2 in other_list:\n",
        "    label = 1 if author_name == author_name2 else 0\n",
        "    if(label==1):\n",
        "      ilist = os.listdir(os.path.join(path,author_name))\n",
        "      lent = len(ilist)\n",
        "      for i in range(lent):\n",
        "        for j in range(i+1,lent):#so that no collisions\n",
        "          img1,img2 = ilist[i],ilist[j]\n",
        "          \n",
        "          imgp_label_list.append([os.path.join(path,author_name,img1),os.path.join(path,author_name2,img2),label])\n",
        "    else:\n",
        "      for img1 in os.listdir(os.path.join(path,author_name)):\n",
        "        for img2 in os.listdir(os.path.join(path,author_name2)):\n",
        "          if(author_name != author_name2 or img1 !=img2):\n",
        "\n",
        "            \n",
        "            imgp_label_list.append([os.path.join(path,author_name,img1),os.path.join(path,author_name2,img2),label])\n",
        "          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXhWUavyBLok",
        "outputId": "b4a037da-c2e7-41c2-ffe2-0e96e7ee4e17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "104767"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(imgp_label_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MlkF0lCtRGnw"
      },
      "outputs": [],
      "source": [
        "class mDataset(Dataset):\n",
        "  def __init__(self,array,transform ):\n",
        "    self.array =array\n",
        "    self.transform = transform\n",
        "    \n",
        "  def __getitem__(self,index):\n",
        "    \n",
        "    \n",
        "        indexlist = self.array[index]\n",
        "        img0 = Image.open(indexlist[0])\n",
        "        \n",
        "        #img0 = img0.convert(\"L\")\n",
        "        img0 = self.transform(img0)\n",
        "        label = indexlist[-1]\n",
        "        \n",
        "        if(len(indexlist)==3):\n",
        "          img1 = Image.open(indexlist[1])\n",
        "          #img1 = img1.convert(\"L\")\n",
        "\n",
        "          img1 = self.transform(img1)\n",
        "          return img0,img1,label\n",
        "        else:\n",
        "          return img0,label\n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SiamesepairwiseDataset(Dataset):\n",
        "  def __init__(self,array,path= None,transform = None):\n",
        "    self.array =array\n",
        "    self.transform = transform\n",
        "    self.path = path\n",
        "    \n",
        "  def __getitem__(self,index):\n",
        "    \n",
        "    indexlist = self.array[index]\n",
        "        \n",
        "    if self.path is None:\n",
        "      img0 = Image.open(indexlist[0])\n",
        "      #img0 = img0.convert(\"L\")\n",
        "\n",
        "      img1 = Image.open(indexlist[1])\n",
        "      #img1 = img1.convert(\"L\")\n",
        "\n",
        "\n",
        "      \n",
        "    else:\n",
        "      img0 = Image.open(os.path.join(self.path,indexlist[0]))\n",
        "      #img0 = img0.convert(\"L\")\n",
        "\n",
        "      img1 = Image.open(os.path.join(self.path,indexlist[1]))\n",
        "      #img1 = img1.convert(\"L\")\n",
        "\n",
        "    \n",
        "    label = indexlist[2]\n",
        "    \n",
        "    if self.transform is not None:\n",
        "        \n",
        "      img0 = self.transform(img0)\n",
        "      img1 = self.transform(img1)\n",
        "    \n",
        "\n",
        "\n",
        "      \n",
        "      \n",
        "    return img0,img1,label\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "mytraintransform = transforms.Compose([transforms.Resize((256,256)),transforms.RandomResizedCrop(224) ,transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])]\n",
        "                               )\n",
        "myvaltransform = transforms.Compose([transforms.Resize((256,256)),transforms.CenterCrop(224) ,transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])]\n",
        "                               )\n",
        "\n",
        "to_tensor = transforms.Compose([transforms.ToTensor(),\n",
        "                               ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HpqVn7aY1nUL"
      },
      "outputs": [],
      "source": [
        "random.shuffle(same_pairs_list)\n",
        "\n",
        "\n",
        "anchor_p_dataset = mDataset(same_pairs_list,transform= mytraintransform)\n",
        "random.shuffle(images_list)\n",
        "n_dataset = mDataset(images_list,transform=mytraintransform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2wOyO6V2lIcD"
      },
      "outputs": [],
      "source": [
        "trainp_dataloader = DataLoader(anchor_p_dataset,batch_size=100)\n",
        "trainn_dataloader = DataLoader(n_dataset,batch_size=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img1_name</th>\n",
              "      <th>img2_name</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>70a4ba9a.jpg</td>\n",
              "      <td>58f68a00.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>e15b612e.jpg</td>\n",
              "      <td>16ce5df2.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6c64d988.jpg</td>\n",
              "      <td>735d3636.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b74681a6.jpg</td>\n",
              "      <td>cb50496d.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>c97aa428.jpg</td>\n",
              "      <td>6f9dc747.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5765</th>\n",
              "      <td>4e7762f7.jpg</td>\n",
              "      <td>2f3d2bce.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5766</th>\n",
              "      <td>c1c49f87.jpg</td>\n",
              "      <td>1072597f.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5767</th>\n",
              "      <td>03e66099.jpg</td>\n",
              "      <td>f877b1b6.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5768</th>\n",
              "      <td>8e056e51.jpg</td>\n",
              "      <td>f68e79a1.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5769</th>\n",
              "      <td>acd11d73.jpg</td>\n",
              "      <td>7eae060a.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5770 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         img1_name     img2_name  label\n",
              "0     70a4ba9a.jpg  58f68a00.jpg      1\n",
              "1     e15b612e.jpg  16ce5df2.jpg      0\n",
              "2     6c64d988.jpg  735d3636.jpg      1\n",
              "3     b74681a6.jpg  cb50496d.jpg      0\n",
              "4     c97aa428.jpg  6f9dc747.jpg      1\n",
              "...            ...           ...    ...\n",
              "5765  4e7762f7.jpg  2f3d2bce.jpg      0\n",
              "5766  c1c49f87.jpg  1072597f.jpg      0\n",
              "5767  03e66099.jpg  f877b1b6.jpg      1\n",
              "5768  8e056e51.jpg  f68e79a1.jpg      1\n",
              "5769  acd11d73.jpg  7eae060a.jpg      0\n",
              "\n",
              "[5770 rows x 3 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(r'D:\\Shreyas\\Shreyas_study\\vision_Challenges\\dataset\\dataset\\val.csv')\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_dataset = SiamesepairwiseDataset(np.array(df),path = r'D:\\Shreyas\\Shreyas_study\\vision_Challenges\\dataset\\dataset\\preprocessedVal',transform=myvaltransform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_dataloader = DataLoader(val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision\n",
        "classification_models = models.list_models(module=models)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(classification_models)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MysR98Wjk-Gp"
      },
      "source": [
        "# Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pretrainednet = models.resnet50(weights=\"DEFAULT\")\n",
        "\n",
        "pretrainednet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "pretrainednet.tail = nn.Sequential(nn.ReLU(inplace=True),nn.Linear(in_features=1000,out_features=128))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "#create a siamese network\n",
        "class SiameseresNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseresNetwork, self).__init__()\n",
        "        # Setting up the Sequential of CNN Layers\n",
        "        self.clf = pretrainednet\n",
        "        \n",
        "    def forward_once(self, x):\n",
        "        # Forward pass \n",
        "        output = self.clf(x)\n",
        "        return output\n",
        "\n",
        "    def forward(self, input1, input2,input3 = None):\n",
        "        # forward pass of input 1\n",
        "        output1 = self.forward_once(input1)\n",
        "        # forward pass of input 2\n",
        "        output2 = self.forward_once(input2)\n",
        "        if(input3 is not None):\n",
        "            output3 = self.forward_once(input3)\n",
        "            return output1, output2,output3\n",
        "        return output1, output2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = SiameseresNetwork()\n",
        "pretrainednet.load_state_dict(torch.load(\"pret.pth\"))\n",
        "net.load_state_dict(torch.load(\"net.pth\"))\n",
        "optimizer =Adam(net.parameters())\n",
        "optimizer.load_state_dict(torch.load(\"optim.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ut0mo3j2EVk",
        "outputId": "d961b095-ed17-4e5a-8c87-1da4f50c432f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [2:27:38, 88.59s/it]\n",
            "5770it [24:05,  3.99it/s]\n",
            " 60%|██████    | 12/20 [16:35<11:03, 82.99s/it]\n",
            " 60%|██████    | 12/20 [16:27<10:58, 82.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy 0.5\n",
            "val F1 0.0\n",
            "val AUC 0.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [2:25:51, 87.52s/it]\n",
            "5770it [24:31,  3.92it/s]\n",
            " 30%|███       | 12/40 [16:42<39:00, 83.58s/it]\n",
            " 30%|███       | 12/40 [16:30<38:30, 82.52s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy 0.5\n",
            "val F1 0.0\n",
            "val AUC 0.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [2:20:18, 84.18s/it]\n",
            "5770it [24:00,  4.00it/s]\n",
            " 20%|██        | 12/59 [16:31<1:04:43, 82.63s/it]\n",
            " 20%|██        | 12/59 [16:40<1:05:18, 83.37s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy 0.5\n",
            "val F1 0.0\n",
            "val AUC 0.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [2:30:25, 90.25s/it]\n",
            "5770it [25:07,  3.83it/s]\n",
            " 15%|█▌        | 12/79 [17:51<1:39:44, 89.32s/it]\n",
            " 15%|█▌        | 12/79 [16:59<1:34:49, 84.92s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy 0.5086655112651647\n",
            "val F1 0.32107496463932106\n",
            "val AUC 0.5008665511265165\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [2:18:16, 82.97s/it]\n",
            "5770it [25:09,  3.82it/s]\n",
            " 12%|█▏        | 12/98 [17:59<2:08:54, 89.94s/it]\n",
            " 12%|█▏        | 12/98 [17:55<2:08:29, 89.65s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy 0.5\n",
            "val F1 0.0\n",
            "val AUC 0.5027729636048527\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [2:30:20, 90.21s/it]\n",
            "5770it [24:16,  3.96it/s]\n",
            " 10%|█         | 12/118 [17:00<2:30:17, 85.07s/it]\n",
            " 10%|█         | 12/118 [17:06<2:31:09, 85.57s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy 0.5\n",
            "val F1 0.0\n",
            "val AUC 0.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [2:42:38, 97.59s/it] \n",
            "5770it [25:48,  3.73it/s]\n",
            "  9%|▉         | 12/137 [18:16<3:10:23, 91.39s/it]\n",
            "  9%|▉         | 12/137 [18:08<3:08:59, 90.71s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy 0.5\n",
            "val F1 0.0\n",
            "val AUC 0.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [2:25:09, 87.09s/it]\n",
            "5770it [24:17,  3.96it/s]\n",
            "  8%|▊         | 12/157 [18:09<3:39:23, 90.78s/it]\n",
            "  8%|▊         | 12/157 [17:41<3:33:40, 88.42s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy 0.5\n",
            "val F1 0.0\n",
            "val AUC 0.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [2:30:18, 90.18s/it]\n",
            "5770it [24:17,  3.96it/s]\n",
            "  7%|▋         | 12/176 [18:00<4:06:11, 90.07s/it]\n",
            "  7%|▋         | 12/176 [17:50<4:03:53, 89.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy 0.5\n",
            "val F1 0.0\n",
            "val AUC 0.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "70it [1:44:58, 89.98s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32md:\\Shreyas\\Shreyas_study\\vision_Challenges\\resnet50_complete.ipynb Cell 51\u001b[0m in \u001b[0;36m<cell line: 166>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Shreyas/Shreyas_study/vision_Challenges/resnet50_complete.ipynb#Y100sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m \u001b[39m#set the device to cuda\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Shreyas/Shreyas_study/vision_Challenges/resnet50_complete.ipynb#Y100sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Shreyas/Shreyas_study/vision_Challenges/resnet50_complete.ipynb#Y100sZmlsZQ%3D%3D?line=165'>166</a>\u001b[0m train()\n",
            "\u001b[1;32md:\\Shreyas\\Shreyas_study\\vision_Challenges\\resnet50_complete.ipynb Cell 51\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Shreyas/Shreyas_study/vision_Challenges/resnet50_complete.ipynb#Y100sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mif\u001b[39;00m(loss_contrastive\u001b[39m.\u001b[39mitem()\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Shreyas/Shreyas_study/vision_Challenges/resnet50_complete.ipynb#Y100sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     loss_contrastive\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Shreyas/Shreyas_study/vision_Challenges/resnet50_complete.ipynb#Y100sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Shreyas/Shreyas_study/vision_Challenges/resnet50_complete.ipynb#Y100sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m train_im1\u001b[39m.\u001b[39mappend([adata,pdata,\u001b[39m1\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Shreyas/Shreyas_study/vision_Challenges/resnet50_complete.ipynb#Y100sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m train_im0\u001b[39m.\u001b[39mappend([adata,ndata,\u001b[39m0\u001b[39m])\n",
            "File \u001b[1;32mc:\\ProgramData\\miniconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[1;32mc:\\ProgramData\\miniconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
            "File \u001b[1;32mc:\\ProgramData\\miniconda3\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
            "File \u001b[1;32mc:\\ProgramData\\miniconda3\\lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
            "File \u001b[1;32mc:\\ProgramData\\miniconda3\\lib\\site-packages\\torch\\optim\\adam.py:393\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m--> 393\u001b[0m param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Declare Siamese Network\n",
        "n_epochs = 1\n",
        "\n",
        "\n",
        "# Decalre Loss Function\n",
        "criterion = TripletMarginWithDistanceLoss(distance_function= lambda t1,t2: 1-torch.cosine_similarity(t1,t2) )\n",
        "# Declare Optimizer\n",
        "#train the model\n",
        "loss=[] \n",
        "counter=[]\n",
        "test_loss = []\n",
        "#for training of knn\n",
        "train_im1 = []\n",
        "train_im0  = []\n",
        "\n",
        "def train():\n",
        "    \n",
        "    net.train()\n",
        "    for data1,data2 in  zip(trainp_dataloader,trainn_dataloader):\n",
        "        \n",
        "        for adata,pdata,l1 in tqdm(zip(data1[0],data1[1],data1[2])):\n",
        "            for  ndata,l2 in zip(data2[0],data2[1]):\n",
        "                \n",
        "                if(l1!=l2):\n",
        "                        \n",
        "                        \n",
        "                        output1,output2,output3 = net(adata[None,:],pdata[None,:],ndata[None,:])#Adding extra dimension\n",
        "                        optimizer.zero_grad()\n",
        "                        \n",
        "                        loss_contrastive = criterion(output1,output2,output3)\n",
        "                        \n",
        "                        \n",
        "                        if(math.isnan(loss_contrastive.item())):\n",
        "                            print(\"Nan\")\n",
        "                            break\n",
        "                        if(loss_contrastive.item()>0):\n",
        "                            loss_contrastive.backward()\n",
        "     \n",
        "                            optimizer.step()\n",
        "                        \n",
        "                         \n",
        "                        train_im1.append([adata,pdata,1])\n",
        "                        train_im0.append([adata,ndata,0])\n",
        "                        \n",
        "                        # else:\n",
        "                        \n",
        "                            \n",
        "                            \n",
        "                        #     trainX.append(1-torch.cosine_similarity(output1,output2).item())\n",
        "                        #     trainy.append(1)\n",
        "                        #     trainX.append(1-torch.cosine_similarity(output1,output3).item())\n",
        "                        #     trainy.append(0)\n",
        "                \n",
        "                            \n",
        "               \n",
        "        \n",
        "\n",
        "        test()\n",
        "        net.train()\n",
        "            \n",
        "  \n",
        "      \n",
        "         \n",
        "           \n",
        "                        \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "def test():\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        testX = np.zeros(5770)\n",
        "        y_true = np.zeros(5770)\n",
        "        for count,i in tqdm(enumerate(val_dataloader),position=0,leave=True):\n",
        "            img0,img1,label = i\n",
        "            output1,output2= net(img0,img1)\n",
        "            testX[count] = 1-torch.cosine_similarity(output1,output2).item()\n",
        "            y_true[count] = label\n",
        "       \n",
        "\n",
        "        batch_size = 512\n",
        "        counter = 0\n",
        "        nextbreak = False\n",
        "        \n",
        "        trainX  = np.zeros(5770*2)\n",
        "        trainy =  np.zeros(5770*2)\n",
        "        random.shuffle(train_im0)\n",
        "        random.shuffle(train_im1)\n",
        "        tdataloader = DataLoader(train_im1,batch_size=512)\n",
        "        for i in tqdm(tdataloader):\n",
        "            if(nextbreak):\n",
        "                break\n",
        "            img0,img1,label = i\n",
        "            \n",
        "      #img0,img1 = img0.cuda(),img1.cuda()\n",
        "      \n",
        "            output1,output2 = net(img0,img1)\n",
        "            dist = 1- torch.cosine_similarity(output1,output2)\n",
        "      \n",
        "            if(counter+batch_size-1<5770):\n",
        "                trainX[counter:counter+batch_size] = dist.cpu()\n",
        "                trainy[counter:counter+batch_size] =  label\n",
        "                counter+=batch_size\n",
        "            else:\n",
        "                trainX[counter:5770] = dist.cpu()[:5770-counter]\n",
        "                trainy[counter:5770] = label[:5770-counter]\n",
        "                nextbreak = True\n",
        "      \n",
        "        counter = 5770\n",
        "        nextbreak = False\n",
        "        \n",
        "        \n",
        "        tdataloader = DataLoader(train_im0,batch_size=512)\n",
        "        for i in tqdm(tdataloader):\n",
        "            if(nextbreak):\n",
        "                break\n",
        "            img0,img1,label = i\n",
        "            \n",
        "      #img0,img1 = img0.cuda(),img1.cuda()\n",
        "      \n",
        "            output1,output2 = net(img0,img1)\n",
        "            dist = 1- torch.cosine_similarity(output1,output2)\n",
        "      \n",
        "            if(counter+batch_size-1<5770*2):\n",
        "                trainX[counter:counter+batch_size] = dist.cpu()\n",
        "                trainy[counter:counter+batch_size] =  label\n",
        "                counter+=batch_size\n",
        "            else:\n",
        "                trainX[counter:] = dist.cpu()[:5770*2-counter]\n",
        "                trainy[counter:] = label[:5770*2-counter]\n",
        "                nextbreak = True\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        trainX = trainX.reshape(-1,1)\n",
        "        testX = testX.reshape(-1,1)\n",
        "        torch.save(pretrainednet.state_dict(),\"pret.pth\")\n",
        "        torch.save(net.state_dict(),\"net.pth\")\n",
        "        knn = KNeighborsClassifier(metric=\"cosine\")\n",
        "        knn.fit(trainX,trainy)\n",
        "        y_pred = knn.predict(testX)\n",
        "        y_prob = knn.predict_proba(testX).T[1].reshape(-1)\n",
        "        \n",
        "        print(\"Train Accuracy\",knn.score(trainX,trainy))\n",
        "        print(\"val F1\",f1_score(y_true,y_pred))\n",
        "        print(\"val AUC\",roc_auc_score(y_true,y_prob))\n",
        "        with open(\"gooddprayswaminarayan.pickle\",\"wb\") as file:\n",
        "            pickle.dump([trainX,trainy,testX,y_true,y_pred,y_prob],file)\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "#set the device to cuda\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(pretrainednet.state_dict(),\"pret.pth\")\n",
        "torch.save(net.state_dict(),\"net.pth\")\n",
        "torch.save(optimizer.state_dict(),\"optim.pt\")\n",
        "with open (\"reversedimagelist.pickle\",'wb') as file:\n",
        "    pickle.dump(images_list,file)\n",
        "with open (\"reversedsamepaires.pickle\",'wb') as file:\n",
        "    pickle.dump(same_pairs_list,file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WnTgUk7PlTiC"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_dataloader = DataLoader(val_dataset,batch_size = 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "l = []\n",
        "l.append(torch.Tensor([[1],2,3])[None,:])\n",
        "l.append(torch.Tensor([1,2,3])[None,:])\n",
        "l.append(torch.Tensor([1,2,3])[None,:])\n",
        "d = DataLoader(l,batch_size=2)\n",
        "for i in d,\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_im"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [17:31<00:00, 87.65s/it]\n"
          ]
        }
      ],
      "source": [
        "testX = np.zeros(5770)\n",
        "testy = np.zeros(5770)\n",
        "with torch.no_grad():\n",
        "  sumdist = 0\n",
        "  counter = 0\n",
        "  batch_size = 512\n",
        "\n",
        "  for i in tqdm(val_dataloader):\n",
        "      \n",
        "      img0,img1,label = i\n",
        "      \n",
        "      #img0,img1 = img0.cuda(),img1.cuda()\n",
        "      \n",
        "      output1,output2 = net(img0,img1)\n",
        "      dist = 1-torch.cosine_similarity(output1,output2)\n",
        "      \n",
        "      if(counter+batch_size-1<5770):\n",
        "        testX[counter:counter+batch_size] = dist.cpu()\n",
        "        testy[counter:counter+batch_size]  = label\n",
        "        counter+=batch_size\n",
        "      else:\n",
        "        testX[counter:] = dist.cpu()\n",
        "        testy[counter:] = label"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(net.state_dict(),\"good.pth\")\n",
        "torch.save(pretrainednet.state_dict(),\"gres.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(imgp_label_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = SiameseNetwork()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vgg.load_state_dict(torch.load(\"vgg.pt\"))\n",
        "net.load_state_dict(torch.load(\"modelpls.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainpdataloader = DataLoader(SiamesepairwiseDataset(imgp_label_list,transform=mytransform),batch_size = 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "sns.kdeplot(trainX[trainy==1],color = \"blue\")\n",
        "sns.kdeplot(trainX[trainy==0],color = \"red\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(metric=&#x27;cosine&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(metric=&#x27;cosine&#x27;)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "KNeighborsClassifier(metric='cosine')"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "knn = KNeighborsClassifier(metric=\"cosine\")\n",
        "knn.fit(trainX.reshape(-1,1),trainy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5015597920277297"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "roc_auc_score(testy,knn.predict_proba(testX.reshape(-1,1)).T[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open (\"d.pickle\",'rb') as file:\n",
        "    [X,y] = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.kdeplot(X[y==0],color='red')\n",
        "sns.kdeplot(X[y==1],color='blue')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "knn = KNeighborsClassifier(metric=\"cosine\",n_neighbors=5)\n",
        "knn.fit(X.reshape(-1,1),y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('testdgood.pickle','wb') as file:\n",
        "    pickle.dump([testX,testy],file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    testX = np.zeros(5770)\n",
        "    y_true = np.zeros(5770)\n",
        "    for count,i in tqdm(enumerate(val_dataloader),position=0,leave=True):\n",
        "        img0,img1,label = i\n",
        "        output1,output2= net(img0,img1)\n",
        "        testX[count] = 1-torch.cosine_similarity(output1,output2).item()\n",
        "            \n",
        "        y_true[count] = label\n",
        "    testX = testX.reshape(-1,1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f = []\n",
        "auc = []\n",
        "for i in tqdm(range(1,16)):\n",
        "    knn = KNeighborsClassifier(metric=\"cosine\",n_neighbors=i)\n",
        "    knn.fit(X.reshape(-1,1),y)\n",
        "    y_pred = knn.predict(testX)\n",
        "    y_prob =knn.predict_proba(testX).T[1].reshape(-1)\n",
        "\n",
        "    f.append(f1_score(y_true,y_pred))\n",
        "    auc.append(roc_auc_score(y_true,y_prob))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(range(15),f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_,cdfprobabilities1 = sns.ecdfplot(distances1).get_lines()[0].get_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.ecdfplot(distances1,color = 'blue')\n",
        "sns.ecdfplot(distances0,color = 'red')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "d0,probabilities0 = sns.kdeplot(distances0).get_lines()[0].get_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "d1,probabilities1 = sns.kdeplot(distances1).get_lines()[0].get_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvJX5Qnekd25"
      },
      "outputs": [],
      "source": [
        "def get_nearest_index(test_distance,array ):\n",
        "    array = np.asarray(array)\n",
        "    \n",
        "    return (np.abs(array-test_distance)).argmin() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKSBRCjYkkdZ"
      },
      "outputs": [],
      "source": [
        "def get_label_and_probability(test_distance ):\n",
        "  idx1 = get_nearest_index(test_distance,d1)\n",
        "  idx0 = get_nearest_index(test_distance,d0)\n",
        "  cdfidx1 = get_nearest_index(test_distance,distances1)\n",
        "  prob0 = probabilities0[idx0]\n",
        "  prob1 = probabilities1[idx1]\n",
        "  label = 1 if prob1>= prob0 else 0\n",
        "  probans = cdfprobabilities1[1:][cdfidx1]\n",
        "  \n",
        "  \n",
        "  \n",
        "  return label,probans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IqBWDP2k6C5"
      },
      "outputs": [],
      "source": [
        "transform=transforms.Compose([transforms.Resize((250,25)),\n",
        "                                                              transforms.ToTensor()\n",
        "                                                                      ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aZOh1RgBpbl"
      },
      "outputs": [],
      "source": [
        "val_dataset = SiamesepairwiseDataset(np.array(df),path = r'D:\\Shreyas\\Shreyas_study\\vision_Challenges\\dataset\\dataset\\val',transform=mytransform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU_Id_3QeKGv"
      },
      "outputs": [],
      "source": [
        "val_dataloader = DataLoader(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqYL_Di_knb7",
        "outputId": "26dff04f-341a-4426-d270-ffc248c0ac18"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  \n",
        "  y_pred = np.zeros(5770)\n",
        "  y_prob  = np.zeros(5770)\n",
        "\n",
        "  for count,i in tqdm(enumerate(val_dataloader)):\n",
        "    img0,img1,_ = i\n",
        "    #img0,img1 = img0.cuda(),img1.cuda()\n",
        "    output1,output2 = net(img0,img1)\n",
        "    dist = 1-torch.cosine_similarity(output1,output2)\n",
        "    (label,prob) = get_label_and_probability(dist.item())\n",
        "    \n",
        "    y_pred[count] = label\n",
        "    y_prob[count] = prob\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpxVSErmMOw3"
      },
      "outputs": [],
      "source": [
        "y_true = df['label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkgZLTmQMQev",
        "outputId": "2ef4e0fa-fbd3-4240-93dc-0c9a366b11a4"
      },
      "outputs": [],
      "source": [
        "f1_score(y_true,y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei6rYr0LMeNl",
        "outputId": "ccbe5902-9f5d-4a3a-cbe4-e66b3a857602"
      },
      "outputs": [],
      "source": [
        "roc_auc_score(y_true,y_prob)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
